{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Within this notebook we will play with clustering.\n",
    " \n",
    " * First try the hierarchical clustering and k-means on artificial data to see visually how those two methods work.\n",
    " * Then, on the Iris dataset we use the hierarchical clustering to observe its structure.\n",
    " * Finally, we focus on the vector quantization as a nice application of k-means algorithm to compress images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "np.set_printoptions(precision=5, suppress=True)  # suppress scientific float notation (so 0.000 is printed as 0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial data - hierarchical clustering & k-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artificial data generation\n",
    "\n",
    "We generate random samples from three different multivariate normal distributions and merge them.\n",
    "\n",
    "The parameters of multivariate normal distribution are:\n",
    "* The mean - corresponding to the center of the cluster.\n",
    "* The covariance matrix - corresponding to the shape (circle or ellipse with some direction) and size of the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate three clusters: a with 60 points, b with 40, c with 20:\n",
    "np.random.seed(50)  # for repeatability of this tutorial\n",
    "a = np.random.multivariate_normal([7, 7], [[2, 0.5], [0.5, 2]], size=[60,])\n",
    "b = np.random.multivariate_normal([0, 15], [[2, 0], [0, 2]], size=[40,])\n",
    "c = np.random.multivariate_normal([15, 0], [[3, 1], [1, 4]], size=[20,])\n",
    "\n",
    "# merge the clusters into X\n",
    "X = np.concatenate((a, b, c),)\n",
    "\n",
    "# print the shape\n",
    "print(X.shape)\n",
    "\n",
    "# visualize the data\n",
    "plt.scatter(X[:,0], X[:,1])\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical agglomerative clustering\n",
    "\n",
    "We use the `scipy` library and especially its hierarchical clustering functions. [(docs here)](https://docs.scipy.org/doc/scipy/reference/cluster.hierarchy.html).\n",
    "\n",
    "This part is inspired by [this blog post](https://joernhees.de/blog/2015/08/26/scipy-hierarchical-clustering-and-dendrogram-tutorial/).\n",
    "\n",
    "#### First we create the dendrogram\n",
    "We use a `linkage` function for that. [(docs here)](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html#scipy.cluster.hierarchy.linkage).\n",
    "* One of its important arguments is a method how the cluster distance is measured.\n",
    "* First we use single linkage, but later you can play with other ones - complete linkage / average linkage / ward's method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "# generate the linkage matrix \n",
    "Z = linkage(X, 'single')\n",
    "\n",
    "# see the shape of the output\n",
    "print(Z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a linkage matrix where each row corresponds to a merging of two clusters. In columns we have:\n",
    "* the index of a first merged cluster\n",
    "* the index of the second merged cluster\n",
    "* the distance between merged clusters\n",
    "* the number of original observations in the newly formed cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe first 5 rows of Z\n",
    "print(Z[:5,:])\n",
    "\n",
    "# especially focus on the 3rd row where the second cluster with index 120 is actually the first one created by merging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let us visualize the dendrogram\n",
    "For this we can use the `dendrogram` function. [(docs here)](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.dendrogram.html#scipy.cluster.hierarchy.dendrogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate full dendrogram\n",
    "plt.figure(figsize=(25, 10))\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('sample index')\n",
    "plt.ylabel('distance')\n",
    "dendrogram(\n",
    "    Z,\n",
    "    leaf_rotation=90.,  # rotates the x axis labels\n",
    "    leaf_font_size=8.,  # font size for the x axis labels\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that:\n",
    "* horizontal lines are cluster merges\n",
    "* vertical lines tell us which clusters/labels were part of merge forming that new cluster\n",
    "* heights of the horizontal lines corresponds to distances between merged clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also plot just the upper part of the dendrogram\n",
    "plt.title('Hierarchical Clustering Dendrogram (truncated)')\n",
    "plt.xlabel('sample index or (cluster size)')\n",
    "plt.ylabel('distance')\n",
    "dendrogram(\n",
    "    Z,\n",
    "    truncate_mode='lastp',  # show only the last p merged clusters\n",
    "    p=10,  # show only the last p merged clusters\n",
    "    leaf_rotation=90.,\n",
    "    leaf_font_size=12.,\n",
    "    show_contracted=True,  # to get a distribution impression in truncated branches\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More fancy version of dendrogram plot\n",
    "With [annotating the distances inside the dendrogram](https://stackoverflow.com/questions/11917779/how-to-plot-and-annotate-hierarchical-clustering-dendrograms-in-scipy-matplotlib/12311618#12311618)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fancy_dendrogram(*args, **kwargs):\n",
    "    max_d = kwargs.pop('max_d', None)\n",
    "    if max_d and 'color_threshold' not in kwargs:\n",
    "        kwargs['color_threshold'] = max_d\n",
    "    annotate_above = kwargs.pop('annotate_above', 0)\n",
    "\n",
    "    ddata = dendrogram(*args, **kwargs)\n",
    "\n",
    "    if not kwargs.get('no_plot', False):\n",
    "        plt.title('Hierarchical Clustering Dendrogram (truncated)')\n",
    "        plt.xlabel('sample index or (cluster size)')\n",
    "        plt.ylabel('distance')\n",
    "        for i, d, c in zip(ddata['icoord'], ddata['dcoord'], ddata['color_list']):\n",
    "            x = 0.5 * sum(i[1:3])\n",
    "            y = d[1]\n",
    "            if y > annotate_above:\n",
    "                plt.plot(x, y, 'o', c=c)\n",
    "                plt.annotate(\"%.3g\" % y, (x, y), xytext=(0, -5),\n",
    "                             textcoords='offset points',\n",
    "                             va='top', ha='center')\n",
    "        if max_d:\n",
    "            plt.axhline(y=max_d, c='k')\n",
    "    return ddata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fancy_dendrogram(\n",
    "    Z,\n",
    "    truncate_mode='lastp',\n",
    "    p=12,\n",
    "    leaf_rotation=90.,\n",
    "    leaf_font_size=12.,\n",
    "    show_contracted=True,\n",
    "    annotate_above=1.9,  # useful in small plots so annotations don't overlap\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retreiving clusters from the dendrogram\n",
    "For this we have the `fcluster` function. [(docs here)](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.fcluster.html#scipy.cluster.hierarchy.fcluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If we know the cut height \n",
    "i.e. the threshold for the cluster distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import fcluster\n",
    "max_d = 5\n",
    "clusters = fcluster(Z, max_d, criterion='distance')\n",
    "# show clusters\n",
    "print(clusters)\n",
    "\n",
    "# figure\n",
    "plt.scatter(X[:,0], X[:,1], c=clusters, cmap='brg')  # plot points with cluster dependent colors\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the cut in the truncated dendrogram\n",
    "fancy_dendrogram(\n",
    "    Z,\n",
    "    truncate_mode='lastp',\n",
    "    p=12,\n",
    "    leaf_rotation=90.,\n",
    "    leaf_font_size=12.,\n",
    "    show_contracted=True,\n",
    "    annotate_above=1.9,  # useful in small plots so annotations don't overlap\n",
    "    max_d = max_d,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If we know the number of clusters $k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "clusters = fcluster(Z, k, criterion='maxclust')\n",
    "# show clusters\n",
    "print(clusters)\n",
    "\n",
    "# figure\n",
    "plt.scatter(X[:,0], X[:,1], c=clusters, cmap='brg')  # plot points with cluster dependent colors\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means algorithm\n",
    "We use `clustering` part of the `sklearn` library. [(docs here)](http://scikit-learn.org/stable/modules/clustering.html#k-means)\n",
    "\n",
    "There is a function `Kmeans` which can be used for that. [(docs here)](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# first lets try 2 clusters\n",
    "k = 2\n",
    "kmeans = KMeans(n_clusters = k, random_state = 1).fit(X)\n",
    "\n",
    "# to show resulting clusters\n",
    "print(kmeans.labels_)\n",
    "# to see cluster centers\n",
    "print(kmeans.cluster_centers_)\n",
    "\n",
    "# figure\n",
    "plt.scatter(X[:,0], X[:,1], c=kmeans.labels_, cmap='brg', alpha=0.4)  # plot points with cluster dependent colors\n",
    "plt.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1], c = 'black', s=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the same for 3 clusters\n",
    "k = 3\n",
    "kmeans = KMeans(n_clusters = k, random_state = 1).fit(X)\n",
    "\n",
    "# figure\n",
    "plt.scatter(X[:,0], X[:,1], c=kmeans.labels_, cmap='brg', alpha=0.4)  # plot points with cluster dependent colors\n",
    "plt.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1], c = 'black', s=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the default setting the algorithm initializes in some smart way, `init = 'k-means++'` [David, Vassilvitskii (2007)](http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf), and the whole run is repeated 10 times, `n_init = 10`.\n",
    "\n",
    "It is also possible to initialize it manually or with random selection from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "# manual initialization\n",
    "initial_centers = np.array([[0,10],[10,10],[10,0]])\n",
    "\n",
    "# clusterring\n",
    "kmeans = KMeans(n_clusters = k, random_state = 1, init = initial_centers, n_init = 1).fit(X)\n",
    "\n",
    "# figure\n",
    "plt.scatter(X[:,0], X[:,1], c=kmeans.labels_, cmap='brg', alpha=0.4)  # plot points with cluster dependent colors\n",
    "plt.scatter(initial_centers[:,0], initial_centers[:,1], c = 'black', s=50, alpha = 0.9, marker = 'x') # initial centers\n",
    "plt.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1], c = 'black', s=100) # final centers\n",
    "plt.legend(['Data', 'Initial Centers', 'Final Centroids'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The $k$-means may be also performed manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances_argmin\n",
    "centers = initial_centers\n",
    "for i in range(3):\n",
    "    y_pred = pairwise_distances_argmin(X, centers)\n",
    "    new_centers = np.array([X[y_pred == i].mean(0) for i in range(len(centers))])\n",
    "\n",
    "    # figure\n",
    "    plt.scatter(X[:,0], X[:,1], c=y_pred, cmap='brg', alpha=0.4)  # plot points with cluster dependent colors\n",
    "    plt.scatter(centers[:,0], centers[:,1], c = 'black', s=50, alpha = 0.9, marker = 'x') # old centers\n",
    "    plt.scatter(new_centers[:,0], new_centers[:,1], c = 'black', s=100) # new centers\n",
    "    plt.title('Manual $k$-means, step ' + str(i+1))\n",
    "    plt.legend(['Data', 'Old Centers', 'New Centers'])\n",
    "    plt.show()\n",
    "    \n",
    "    centers = new_centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elbow method for $k$ estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = np.zeros(5)\n",
    "iy = np.zeros(5)\n",
    "for k in range(ix.shape[0]):\n",
    "    kmeans = KMeans(n_clusters=k+1, random_state = 1)\n",
    "    kmeans.fit(X)\n",
    "    iy[k] = kmeans.inertia_\n",
    "    ix[k] = k+1\n",
    "\n",
    "plt.xlabel('$k$')\n",
    "plt.ylabel('Objective function')\n",
    "plt.plot(ix, iy, 'o-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - perform hierarchical clustering on Iris dataset\n",
    "* Search for 3 clusters\n",
    "* Discuss and measure somehow the quality of final clusters. The real types of irises’ (Setosa, Versicolour, and Virginica) are stored in y variable.\n",
    "* Try to find a cluster distance function such that the final clusters correspond to real types as accurate as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "print('X shape:', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here\n",
    "\n",
    "Z = linkage(X, 'single') # best is ward - at least between the triple - single, complete and ward\n",
    "k = 3\n",
    "clusters = fcluster(Z, k, criterion='maxclust')\n",
    "# recode values so they match - this have to be done manually\n",
    "print('Assigned clusters:', clusters)\n",
    "print('True types:', y)\n",
    "# - ward method & single linkage\n",
    "clusters[clusters == 1] = 0\n",
    "clusters[clusters == 3] = 1\n",
    "\n",
    "# - complete linkage\n",
    "# clusters[clusters == 3] = 0\n",
    "# clusters[clusters == 2] = 3\n",
    "# clusters[clusters == 1] = 2\n",
    "# clusters[clusters == 3] = 1\n",
    "\n",
    "# show accuracy\n",
    "print('Accuracy:', 1 - np.abs(clusters - y).sum()/y.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 - perform vector quantization with $k$-means on a figure\n",
    "You need to install Pillow package\n",
    "`pip install Pillow`. [(docs here)](https://pillow.readthedocs.io/en/5.3.x/index.html)\n",
    "\n",
    "First some code that brings you to the task itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "# open and conver to grayscale\n",
    "im = Image.open(\"figure.jpg\").convert(\"L\")\n",
    "# covert to numpy array of numbers between 0 and 1\n",
    "pix = np.array(im)/255.0\n",
    "print('Shape of the array:', pix.shape)\n",
    "# visualize\n",
    "plt.imshow(pix, cmap=\"gray\", clim=(0, 1));\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the task:\n",
    "* crop the image width to be a multiply of 4\n",
    "* create column blocks of length 4 - i.e. subparts from the original image of the shape (1,4)\n",
    "* perform k-means clusterring with k = 255 - i.e. one byte will be sufficient to transmit the cluster indices\n",
    "* extract centroids and labels from the clustering\n",
    "* decode them back to the array of the original shape - **hint** -  use: `restored = np.take(centroids, labels, axis = 0)`\n",
    "* visualize the result\n",
    "* discuss the size reduction (compression) when one uses the centroids and lables instead of the original pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here\n",
    "\n",
    "\n",
    "# set block length to 4\n",
    "block_len = 4\n",
    "\n",
    "# crop the image to optimal size\n",
    "rows,cols = pix.shape\n",
    "pix = pix[:, :(cols - cols % block_len)]\n",
    "\n",
    "# reshape the figure to create the blocks\n",
    "X = pix.reshape(-1, block_len)\n",
    "print(X.shape)\n",
    "\n",
    "# perform clusterring\n",
    "k = 255 # i.e. one byte will be sufficient for transfering the values\n",
    "kmeans = KMeans(n_clusters = k, random_state = 1, n_init = 2).fit(X)\n",
    "\n",
    "# extract the centroids and labels\n",
    "centroids = kmeans.cluster_centers_\n",
    "labels = kmeans.labels_\n",
    "print(centroids.shape)\n",
    "print(labels.shape)\n",
    "\n",
    "# restore the compressed image from centroids and labels\n",
    "restored = np.take(centroids, labels, axis = 0)\n",
    "final_pix = restored.reshape(pix.shape)\n",
    "\n",
    "# visualize a result\n",
    "plt.imshow(final_pix, cmap=\"gray\", clim=(0, 1));\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
